name: TemporalFusionTransformer

model_cfg:
  # Architecture
  hidden_size: 32
  lstm_layers: 1
  attention_head_size: 1
  dropout: 0.1
  hidden_continuous_size: 16
  output_size: 7

  # logging
  log_interval: 2

  # Optimization
  learning_rate: 0.03
  reduce_on_plateau_patience: 4
